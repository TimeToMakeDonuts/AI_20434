{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931bfc9e-71be-43b4-b084-c3e112d43011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77adf2d-73e6-4f82-8ae8-5b96f1b115c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to learn about transformer technology. Let's start by thinking about what I already know. Transformers are a type of machine learning model used in natural language processing (NLP), right? They're pretty popular for tasks like translation and text generation.\n",
      "\n",
      "I remember that transformers use something called self-attention. That means each word in the input can see every other word in the sequence. This helps them understand context better, which is why they work well with sentences where words need to relate to each other globally, not just locally.\n",
      "\n",
      "The key part of a transformer is the attention mechanism, which is based on something called a query, key, and value. These are learned during training so that when you process each word, it can compute how important it is relative to others. This must help in capturing relationships between words.\n",
      "\n",
      "Then there's the positional encoding. I think this is necessary because transformers don't have inherent understanding of order or position. So they need a way to encode where each word is in the sequence. Positional encoding adds numerical values to each word's position, allowing the model to recognize that earlier words come before later ones.\n",
      "\n",
      "The architecture itself has layers called transformer blocks. Each block consists of multiple attention heads. These heads can focus on different parts of the input data simultaneously, which should make the model more robust and able to capture various patterns in the data.\n",
      "\n",
      "When processing a sequence, the tokens (words or other units) go through each layer, passing information forward with both self-attention and feed-forward neural networks. This multi-layer structure allows the model to learn complex patterns and representations.\n",
      "\n",
      "Training transformers requires optimizing using a loss function, like cross-entropy for classification tasks. The optimizer then adjusts the model's parameters to minimize this loss. It must be computationally intensive because of all these parameters and layers, which is why they often require GPUs or TPUs.\n",
      "\n",
      "Applications include translation, text summarization, question answering, and generating text. The flexibility makes them useful in many areas where context understanding is key.\n",
      "\n",
      "I should also think about how transformers differ from other models like RNNs or CNNs. Since transformers process all parts of the input at once, they can handle long-range dependencies better than RNNs which process sequentially. They don't have the same fixed kernel sizes as CNNs, making them more adaptable to sequential data.\n",
      "\n",
      "Common issues with transformers might include their high computational cost, especially for larger models and datasets. This could limit their use in scenarios with limited resources or when dealing with very large datasets without enough data. Also, there's a risk of overfitting if not properly regularized.\n",
      "\n",
      "In summary, transformers are powerful because they can model long-range dependencies, capture complex patterns, and handle various language tasks effectively, though they come with higher computational demands.\n",
      "</think>\n",
      "\n",
      "Transformers are a groundbreaking advancement in machine learning, particularly within the domain of natural language processing (NLP). Here's an organized overview of transformer technology:\n",
      "\n",
      "1. **Conceptual Foundation**:\n",
      "   - Transformers are designed for sequence modeling, excelling at understanding context and relationships between words in text.\n",
      "   - They utilize self-attention mechanisms, allowing each word to interact with every other word in the input sequence.\n",
      "\n",
      "2. **Mechanics**:\n",
      "   - **Attention Mechanism**: Based on query, key, and value vectors, which are learned during training. This mechanism helps in capturing how important each word is relative to others.\n",
      "   - **Positional Encoding**: Essential for maintaining order and position information since transformers don't inherently understand sequence position. This is done by adding numerical values to each token's position.\n",
      "\n",
      "3. **Architecture**:\n",
      "   - Composed of transformer blocks, each containing multiple attention heads. These heads process different parts of the input data simultaneously, enhancing robustness.\n",
      "   - Each layer processes tokens through self-attention and feed-forward neural networks, allowing the model to learn complex patterns and representations across layers.\n",
      "\n",
      "4. **Training**:\n",
      "   - Utilizes loss functions like cross-entropy for tasks such as classification. Optimizers adjust parameters to minimize this loss, often requiring significant computational resources due to their complexity.\n",
      "\n",
      "5. **Applications**:\n",
      "   - Widely used in translation, text summarization, question answering, and text generation. Their flexibility makes them suitable for various context-dependent tasks.\n",
      "\n",
      "6. **Comparison with Other Models**:\n",
      "   - Unlike RNNs, which process data sequentially, transformers handle all input parts simultaneously, better capturing long-range dependencies.\n",
      "   - They differ from CNNs by not having fixed kernel sizes, allowing them to adapt well to sequential data.\n",
      "\n",
      "7. **Considerations**:\n",
      "   - High computational cost, especially for larger models and datasets, necessitating powerful hardware like GPUs or TPUs.\n",
      "   - Risk of overfitting if not properly regularized, requiring careful tuning and techniques to prevent.\n",
      "\n",
      "In essence, transformers are powerful tools for modeling complex language tasks due to their ability to capture long-range dependencies and context. However, their computational demands must be managed appropriately.\n"
     ]
    }
   ],
   "source": [
    "message = \"Tell me something about transformer technology\"\n",
    "response = ollama.chat(model='deepseek-r1:8b',messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c343aeb3-94ea-405e-ab5f-2abcf21b22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Moilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/573.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    def __init__(self,url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\",\"style\",\"img\",\"input\",\"link\",\"ul\",\"li\",\"a\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text= soup.body.get_text(separator=\"\\n\",strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa2dc30-3e0a-4201-8744-28baa8aa283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Duży model językowy – Wikipedia, wolna encyklopedia\n",
      "text:  Menu główne\n",
      "Menu główne\n",
      "przypnij\n",
      "ukryj\n",
      "Nawigacja\n",
      "Dla czytelników\n",
      "Dla wikipedystów\n",
      "Szukaj\n",
      "Wygląd\n",
      "Narzędzia osobiste\n",
      "Strony dla anonimowych edytorów\n",
      "Spis treści\n",
      "przypnij\n",
      "ukryj\n",
      "Przełącz stan spisu treści\n",
      "Duży model językowy\n",
      "55 języków\n",
      "polski\n",
      "Narzędzia\n",
      "Narzędzia\n",
      "przypnij\n",
      "ukryj\n",
      "Działania\n",
      "Ogólne\n",
      "Drukuj lub eksportuj\n",
      "W innych projektach\n",
      "Wygląd\n",
      "przypnij\n",
      "ukryj\n",
      "Z Wikipedii, wolnej encyklopedii\n",
      "Duży model językowy\n",
      "(ang.\n",
      "large language model\n",
      ",\n",
      "LLM\n",
      ")\n",
      "– model\n",
      "umożliwiający generowanie tekstu oraz realizację zadań związanych z\n",
      ". Modele\n",
      "LLM\n",
      "są szkolone w ramach\n",
      "lub słabo nadzorowanego\n",
      "z wykorzystaniem dużych ilości danych tekstowych. Proces ten jest intensywny obliczeniowo\n",
      ". Duże modele językowe mogą być wykorzystywane do generowana tekstu poprzez wielokrotne przewidywanie następnego tokenu lub słowa, przez co zaliczane są do\n",
      ".\n",
      "Duże modele językowe są\n",
      ". Największe i najbardziej zdolne modele językowe oparte są na architekturze\n",
      ".\n",
      "Przykładami dużych modeli językowych są modele z serii\n",
      "zbudowane przez\n",
      "(np.\n",
      "﻿\n",
      ",\n",
      "﻿\n",
      "), używane w chatbotach\n",
      "i Microsoft Copilot, a także modele\n",
      "zbudowane przez\n",
      ". Istnieją również chińskie modele jak\n",
      "czy polskie jak\n",
      "i\n",
      ".\n",
      "Historia\n",
      "[\n",
      "|\n",
      "]\n",
      "Przed rokiem 2017 istniało kilka modeli językowych, które — jak na ówczesne możliwości — były uważane za duże. W latach 90.\n",
      "pozwoliły na rozwinięcie się metod statystycznego modelowania języka. W 2001 roku model bazujący na\n",
      "został wytrenowany na 300 milionach słów\n",
      ". W latach dwutysięcznych, wraz z popularyzacją\n",
      ", rozpoczęto tworzenie zbiorów językowych o skali porównywalnej z Internetem\n",
      ", na bazie których podjęto próby uczenia statystycznych modeli językowych\n",
      ".\n",
      "Po tym, jak sieci neuronowe stały się popularne w przetwarzaniu obrazów około 2012 roku\n",
      ", zaczęto je również stosować do przetwarzania tekstu. W 2016 roku Google wprowadziło w\n",
      "swój model językowy oparty na\n",
      ".\n",
      "Wykres pokazujący łączną ilość obliczeń (we\n",
      "-sach) wymaganą do wytrenowania modelu w kolejnych latach dla wybranych dużych modeli AI. Większość dużych modeli to modele językowe lub mutlimodalne ze zdolnościami przetwarzania języka.\n",
      "W 2017 roku naukowcy z Google zaproponowali architekturę\n",
      "opartą na\n",
      "opracowanym w 2014 roku\n",
      ". W 2018 roku Google wypuściło model BERT oparty wyłącznie na\n",
      ". Od 2023 roku akademickie i badawcze zainteresowanie BERT-em zaczęło stopniowo maleć na rzecz modeli opartych na\n",
      "jak\n",
      ".\n",
      "Od 2022 roku zyskują na popularności modele o otwartym kodzie źródłowym – początkowo za sprawą projektów takich jak BLOOM\n",
      "i\n",
      ", choć oba objęte są ograniczeniami dotyczącymi zakresu zastosowań. Modele\n",
      "udostępnione zostały na bardziej liberalnej licencji\n",
      ". W styczniu 2025 roku firma\n",
      "wypuściła w formie otwartego kodu model DeepSeek R1, mający 671 miliardów parametrów. Jego wydajność jest porównywalna z modelem OpenAI o1, przy znacznie niższych kosztach eksploatacji\n",
      ".\n",
      "Od 2023 roku wiele modeli ma charakter\n",
      ", co oznacza, że potrafią analizować i generować różne typy danych, takie jak tekst, obrazy czy dźwięk\n",
      ".\n",
      "W roku 2024 największe i najbardziej zaawansowane modele językowe oparte są na architekturze transformatora. Niektóre nowsze implementacje wykorzystują jednak inne podejścia, takie jak\n",
      "(RNN) czy\n",
      "﻿\n",
      ".\n",
      "Trenowanie i architektura\n",
      "[\n",
      "|\n",
      "]\n",
      "Zobacz też:\n",
      ".\n",
      "Wzmacnianie z informacją zwrotną od człowieka\n",
      "[\n",
      "|\n",
      "]\n",
      "to metoda umożliwiająca dostosowanie działania modelu językowego do ludzkich oczekiwań. Preferencje użytkowników definiuje się poprzez trenowanie tzw. modelu nagród, który następnie służy do dalszego uczenia modelu językowego z wykorzystaniem algorytmów\n",
      "takich jak\n",
      "proximal policy optimization\n",
      "(PPO)\n",
      ".\n",
      "Mieszanka ekspertów\n",
      "[\n",
      "|\n",
      "]\n",
      "Zobacz też:\n",
      ".\n",
      "Największe modele językowe bywają zbyt kosztowne w trenowaniu i bezpośrednim zastosowaniu. Dlatego coraz częściej stosuje się podejście mieszanki ekspertów (ang. Mixture of Experts, MoE)\n",
      ". MoE to technika, która dzieli przestrzeń problemu między wiele wyspecjalizowanych\n",
      "przez co inferencja aktywuje tylko te części sieci, które są najbardziej odpowiednie\n",
      ", a sama metoda jest zaliczana do metod\n",
      ".\n",
      "Dostrajanie na podstawie instrukcji\n",
      "[\n",
      "|\n",
      "]\n",
      "Duże modele językowe uczą się generować poprawne odpowiedzi i zastępować naiwne uzupełnienia dzięki kilku wstępnym korektom wprowadzonym przez człowieka oraz zastosowaniu podejścia\n",
      "self‑instruct\n",
      ". Na przykład w odpowiedzi na polecenie: „\n",
      "Napisz esej na temat głównych motywów przedstawionych w Hamlecie\n",
      "” model mógłby najpierw wygenerować: „\n",
      "Jeśli oddasz esej po 17 marca, Twoja ocena zostanie obniżona o 10% za każdy dzień opóźnienia\n",
      "”, bazując na częstotliwości takiego ciągu w\n",
      ".\n",
      "Koszt\n",
      "[\n",
      "|\n",
      "]\n",
      "Szacunkowy koszt trenowania wybranych modeli AI\n",
      "Trenowanie i działanie dużych modeli językowych zazwyczaj wymaga ogromnej\n",
      "i zużycia\n",
      ", co rodzi pytania dotyczące wpływu na\n",
      ".\n",
      "Rozumowanie\n",
      "[\n",
      "|\n",
      "]\n",
      "Pod koniec 2024 roku w rozwoju dużych modeli językowych pojawił się nowy kierunek, skoncentrowany na zadaniach wymagających złożonego\n",
      ". Modele tego typu, określane jako „modele rozumujące”, zostały wytrenowane tak, aby poświęcać więcej czasu na generowanie rozwiązań krok po kroku (ang. chain-of-thought) przed udzieleniem odpowiedzi końcowej – w sposób zbliżony do ludzkiego procesu rozwiązywania problemów\n",
      ". Trend ten zapoczątkowała firma OpenAI, wprowadzając we wrześniu 2024 roku model o1\n",
      ", a następnie model o3 w grudniu 2024\n",
      ". W porównaniu z tradycyjnymi LLM-ami, nowe modele wykazywały znaczną poprawę wyników w zadaniach z matematyki, nauk ścisłych oraz programowania. Przykładowo, na zadaniach z eliminacji do\n",
      "model GPT-4o uzyskał 13% skuteczności, podczas gdy model o1 osiągnął aż 83%\n",
      ".\n",
      "W styczniu 2025 roku chińska firma\n",
      "przedstawiła model DeepSeek‑R1 — model rozumowania o otwartych wagach, posiadający 671 miliardów parametrów, który osiągnął wyniki porównywalne do modelu o1 firmy OpenAI, przy znacznie niższych kosztach operacyjnych. W przeciwieństwie do zastrzeżonych modeli OpenAI, otwarta architektura DeepSeek‑R1 umożliwiła badaczom analizę i dalszy rozwój algorytmu, choć dane treningowe pozostały nieupublicznione\n",
      ".\n",
      "Tego typu podejście z reguły mają większe wymagania obliczeniowe w porównaniu z bezpośrednim podejściem ponieważ model musi generować wiele odpowiedzi dla każdego kroku jednak pozwala to na osiągnięcie lepszych wyników w dziedzinach wymagających myślenia domenowego\n",
      ". Aby zmniejszyć ilość występowania\n",
      ", stosowane są dodatkowe techniki jak\n",
      ",\n",
      "czy\n",
      ".\n",
      "Oddziaływanie\n",
      "[\n",
      "|\n",
      "]\n",
      "Istnieją opinie twierdzące, że nie ma możliwości rozróżnienia tekstu stworzonego przez duży model językowy i przez człowieka\n",
      ". Goldman Sachs w 2023 roku twierdził, że LLM-y są w stanie zwiększyć globalne\n",
      "o 7% w ciągu dekady i bedą potrafiły wystawić na automatyzację pracę 300 mln osób\n",
      ".\n",
      "Prawa autorskie\n",
      "[\n",
      "|\n",
      "]\n",
      "W roku 2023, do sądów w Stanach Zjednoczonych wpłynęło kilka wniosków podważających używanie danych chronionych\n",
      "do trenowania modeli językowych z obrońcami opierającymi się na instytucję\n",
      ".\n",
      "Bezpieczeństwo\n",
      "[\n",
      "|\n",
      "]\n",
      "Duże modele językowe mogą być używane do tworzenia\n",
      ", w sposób świadomy lub nie lub do innych celów\n",
      ". Dostępność dużych modeli językowych może pozwolić na obniżenie poziom umiejętności wymaganych do popełnienia czynów\n",
      ".\n",
      "Dodatkowo, istnieje możliwość osadzenia uśpionych\n",
      ", czyli ukrytych funkcjonalności, które w normalnych warunkach nie wykonują akcji, a po uzyskaniu impulsu aktywującego, rozpoczynają wykonywania szkodliwych działań\n",
      ".\n",
      "Aplikacje LLM zawierają odpowiednie filtry moderacyjne jednak nie są one w pełni efektywne i pozwalają na wykorzystanie jako\n",
      "czy różnych nielegalnych operacji\n",
      ".\n",
      "Stronniczość algorytmiczna\n",
      "[\n",
      "|\n",
      "]\n",
      "Podczas gdy duże modele językowe są w stanie generować tekst przypominający ludzki, są skłonne do dziedziczenia i powiększania stronniczości zawartej w\n",
      ". Stronniczość może się objawiać w błędnym i niesprawiedliwym traktowaniu różnych grup demograficznych\n",
      ".\n",
      "Zobacz też\n",
      "[\n",
      "|\n",
      "]\n",
      "Przypisy\n",
      "[\n",
      "|\n",
      "]\n",
      "Linki zewnętrzne\n",
      "[\n",
      "|\n",
      "]\n",
      "Pojęcia\n",
      "Aplikacje\n",
      "Implementacje\n",
      "Tekst do obrazu\n",
      "Tekst do wideo\n",
      "Inne\n",
      "Architektury\n",
      "(\n",
      "model językowy\n",
      "):\n",
      ":\n",
      "Źródło: „\n",
      "”\n",
      ":\n",
      "Ukryte kategorie:\n",
      "Szukaj\n",
      "Szukaj\n",
      "Przełącz stan spisu treści\n",
      "Duży model językowy\n",
      "55 języków\n"
     ]
    }
   ],
   "source": [
    "web = Website(\"https://pl.wikipedia.org/wiki/Du%C5%BCy_model_j%C4%99zykowy\")\n",
    "print(\"Title: \", web.title)\n",
    "print(\"text: \", web.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e9b7626-d5b3-4e8e-aa6d-4850c9220d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website\\\n",
    "and provides a short summary, ignoring text that might be navigation related.\\\n",
    "Respond in markdown in Polish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ae44d94-ae8c-483c-b9b7-cf6a48807204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "    please provide a short summary of this website in Polsih in markdown. \\\n",
    "    If it includes news or announcements, then summarize these in Polish too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec959f11-441e-4adb-afce-d32a06f95d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\",\"content\":system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74bab4b6-d08c-46bd-bc88-53a8c0d82044",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL='deepseek-r1:8b'\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama.chat(\n",
    "        model=MODEL,\n",
    "        messages=messages_for(website)\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a46a9ec9-3568-4355-92ce-2cdf32a495b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, let me try to figure out how to approach this user\\'s query. They want a summary of MyAnimeList.net in Polish, ignoring navigation text. The response should be in markdown.\\n\\nFirst, I\\'ll start by identifying the main sections of the website. It has categories like Anime, Manga, Characters, etc., as well as news and featured articles. The latest news includes announcements about anime adaptations, which are important to include since the user mentioned summarizing those.\\n\\nNext, I need to translate this information into Polish. I\\'ll make sure to structure it in markdown with headers for each section: \"Strona główna\", \"Kategorii\", \"Nowości\", and \"Rekomendacje\". Under each category, I\\'ll list the relevant subcategories.\\n\\nFor the news, I\\'ll pick the most recent ones mentioned, like the adaptation of \"Jishou Akuyaku Reijou na Konyakusha no Kansatsu Kiroku\" by Shiki and the winners of Kodansha\\'s manga awards. I\\'ll summarize each announcement concisely in Polish.\\n\\nIn the anime recommendations section, I\\'ll list a few examples based on the user\\'s content, like \"Tokyo Revengers\" and others, providing brief reasons why they\\'re recommended.\\n\\nI also need to ensure that the markdown formatting is correct, using headers and bullet points where necessary. No navigation links or text should be included, so I\\'ll focus solely on the content sections provided.\\n\\nFinally, I\\'ll review the summary to make sure it\\'s accurate and flows well in Polish, ensuring all key aspects of MyAnimeList are covered succinctly.\\n</think>\\n\\n```markdown\\n## Strona główna\\n\\nMyAnimeList.net to największy portal animowego i mangowego źródła informacji oraz społeczności. Portal zawiera obszarne kategorie takie jak:\\n\\n### Kategorii\\n- Anime\\n- Manga\\n- Postaci\\n- Ludzie\\n- Spółki\\n- Sklep mangowy\\n- Nowości\\n- Artykuły wybitni\\n- Forum\\n- Kluby\\n- Używający\\n\\n---\\n\\n## Nowości\\nW sekcji \"Nowości\" portalu publikowane są aktualności z świata animu i mangy:\\n\\n- **Ogłoszenie adaptacji anime na podstawie powieści \"Jishou Akuyaku Reijou na Konyakusha no Kansatsu Kiroku\"**: Spółka AlphaPolis ogłosiła oficjalną stronę internetową dla telewizyjnej adaptacji tej light novel.\\n- **Wygażenie laureatów nagród mangowych Kodansha**: Wiosenne edycja nagród 49 roku wygrały nowości takie jak \"Wind Breaker\" i inne prace z kategorii shounen.\\n\\n---\\n\\n## Rekomendacje\\nW sekcji \"Rekomendacje\" użytkownicy dzielą się na temat najbardziej wartościowych anime i mang:\\n\\n- **\"Tokyo Revengers\"**: Serial, w którym główny bohater przystaje do walki ze złymi ludźmi po tym, jak mu udało się przeżyć kataklizm.\\n- **Inne rekomendacje**: takie jak seriale oparte na gangach i przyjaciościach z pasją do muzyki.\\n\\n---\\n\\n## Promocje\\nW sekcji \"Promocje\" portalu prezentowane są aktualności wydarzeń społecznych oraz wyświetlania nowych trailerów dla popularnych anime.\\n\\n- Wyświetlanie pierwszego epizodu nowej serii\\n- Panel dyskusyjny zespolu EGOISTS\\n- Oryginalne animacje firmy GoHands\\n\\n---\\n\\n## Ěmiejskość użytkownika\\nPortal pozwala użytkownikom tworzyć własne listy anime i mang, śledzić aktualności i uczestniczyć w dyskusjach w forum. Możesz też utworzyć klub dla swoich ulubieńców.\\n\\n---\\n\\nMyAnimeList.net to niezawodny źródło informacji dla międzyjętnego społeczności animowego i mangowego, z aktualnymi wiadomościami, rekomendacjami i możliwością współpracy.\\n```'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://myanimelist.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26d8a6ab-dd39-4994-81a8-c54eacdd646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4800f76d-f740-4c48-8a20-8b3bedc56529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I need to summarize this website in Polish using markdown. Let me start by reading through the content provided.\n",
       "\n",
       "The website is titled MyAnimeList.net - Anime and Manga Database and Community. The main sections include Anime, Manga, Characters, People, Companies, Manga Store, News, Featured Articles, Forum, Clubs, Users, and a welcome message.\n",
       "\n",
       "Looking at the news section, there are several announcements. For example, Publishing company AlphaPolis opened an official website for a TV anime adaptation of Shiki's Jishou Akuyaku Reijou na Konyakusha no Kansatsu Kiroku. Kodansha announced winners of their 49th annual manga awards, and Bushiroad opened a site for a TV anime adaptation of Atekichi's light novel. There are also updates on additional cast, theme songs, key visuals, and promotional videos for another anime.\n",
       "\n",
       "There are featured articles about the MAL x Honeyfeed Writing Contest 2024 finalists, themed booths at events, episode premieres, and original anime releases by GoHands. The latest anime reviews and recommendations are also mentioned, with users discussing various shows like \"The Rising of the Shield Hero,\" \"Bleach,\" and others.\n",
       "\n",
       "In terms of structure, I should probably list the main sections in Polish, then provide a brief summary highlighting the key features like database, community aspects, news updates, and user interactions. I need to make sure to mention the specific news items if they are significant enough for inclusion in the summary.\n",
       "\n",
       "I also notice that the website offers tracking functionalities, as mentioned in the \"It’s time to ditch the text file\" part, so including something about user lists or tracking features would be relevant.\n",
       "\n",
       "Finally, I should end with the site's copyright information and any disclaimers, but since the content provided doesn't include those beyond the welcome message and the footer, maybe just mentioning the website's purpose in terms of community and database is sufficient.\n",
       "\n",
       "I need to ensure that the Polish summary is concise, captures all key aspects, and follows proper markdown formatting with headers for sections like \"Obszary Strony,\" \"Nowości,\" etc.\n",
       "</think>\n",
       "\n",
       "# MyAnimeList.net - Anime i Manga. Baza Danych i Wspólnota\n",
       "\n",
       "## Obszar Strony\n",
       "- **Anime**\n",
       "- **Manga**\n",
       "- **Postaci**\n",
       "- ** Ludzie**\n",
       "- ** Spółki**\n",
       "- ** Sklepy Mangi**\n",
       "- ** Nowości**\n",
       "- ** Artykuły Specjalne**\n",
       "- ** Forum**\n",
       "- ** Kluby**\n",
       "- ** Użytkownicy**\n",
       "\n",
       "## Wprowadzenie\n",
       "Dobrze, że nasz adres URL to MyAnimeList.net - bazowy serwis dla anime i mangi z społecznością.\n",
       "\n",
       "### Główne cechy:\n",
       "1. **Baza danych**: Duże archiwum anime i mangi z różnymi sezonami emitowanych seriali.\n",
       "2. **Społeczność**: Forum, kluby oraz możliwość tworzenia własnych listów o ulubionych postaciach.\n",
       "3. ** Nowości**: Aktualności z świata anime i mangi, w tym adaptacje filmowe i nowe wydawnictwa.\n",
       "\n",
       "## Nowości\n",
       "- Publishing company AlphaPolis ogłosił oficjalną stronę internetową dla TV anime adaptacji light novel \"Jishou Akuyaku Reijou na Konyakusha no Kansatsu Kiroku\".\n",
       "- Kodansha ogłosił zwyciętki swojej 49. rocznej nagród manga.\n",
       "- Bushiroad założył stronę internetową dla TV anime adaptacji light novel \"Heroine? Seijo? No, I'm an All-Works Maid\".\n",
       "\n",
       "### Dodatkowe informacje:\n",
       "- Nowe obszarowanie aktorsów, tematyki ścieżki i wizualizacje dla anime \"Silent Witch: Chinmoku no Majo no Kakushigoto\".\n",
       "- Wydawnictwa GoHands zaprezentowały oryginalną animację pierwszego episu z alternatywną sequencją otwarcia.\n",
       "\n",
       "## Wydarzenia i Spotkania\n",
       "- Themed Booth i panel \"Meet the EGOISTS at AX!\" wzbudzają zainteresowanie kolejnym sezonem.\n",
       "- Premiera pierwszego episu z uwagi ze strony zespołu, który zapoznaje widownię z reszty serii.\n",
       "\n",
       "## Recenzje Anime\n",
       "- Posiada się liczba recenzji dla różnych seriali, takich jak \"The Rising of the Shield Hero\" i \"Bleach\", gdzie użytkownicy omawiają charakterów i rozwój przyporządkowane seriale.\n",
       "- Polecenia dotyczące podobnych seriali są wystawiane na podstawie uwagi użytkowników.\n",
       "\n",
       "## Funkcja śledzenia\n",
       "- Możliwość tworzenia własnej listy anime z ułatwieniami w organizacji i wyszukiwaniu ulubionych postaci.\n",
       "\n",
       "---\n",
       "\n",
       "MyAnimeList.net to istotnie źródło informacji dla fanów anime i mangi z ogólnym dostępem do danych, aktualnościami i społecznością."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://myanimelist.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c39c5b-b237-489d-bada-ddf808c6857f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
